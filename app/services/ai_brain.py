"""
Cerebro IA - AI Brain Service
Handles intelligent responses using Google Gemini AI for general inquiries.
"""

import logging
import os
from typing import Optional, Dict, Any

logger = logging.getLogger(__name__)

# Try to import Vertex AI
try:
    import vertexai
    from vertexai.generative_models import (
        GenerativeModel,
        Tool,
        FunctionDeclaration,
        Content,
        Part,
        GenerationConfig
    )
    VERTEX_AI_AVAILABLE = True
except ImportError:
    VERTEX_AI_AVAILABLE = False
    logger.warning("âš ï¸  Vertex AI not available, using fallback responses")


class CerebroIA:
    """
    AI Brain for intelligent conversation handling.
    
    Uses Google Gemini 2.0 Flash model via Vertex AI to generate
    contextual responses for general inquiries about motorcycles,
    services, and dealership information.
    """
    
    def __init__(self, config_loader=None, catalog_service=None):
        """
        Initialize the AI brain.
        
        Args:
            config_loader: Optional ConfigLoader instance for dynamic personality
            catalog_service: Optional CatalogService instance for tool use
        """
        self.config_loader = config_loader
        self.catalog_service = catalog_service 
        self.motor_financiero = None # Will be injected
        self._model = None
        self._system_instruction = self._get_system_instruction()
        self.tools = self._create_tools()
        
        # Initialize Vertex AI if available
        if VERTEX_AI_AVAILABLE:
            try:
                vertexai.init(project="tiendalasmotos", location="us-central1")
                # Initialize model WITH tools
                self._model = GenerativeModel(
                    "gemini-2.5-flash",
                    tools=[self.tools] if self.tools else []
                )
                logger.info(f"ğŸ§  CerebroIA initialized with Gemini 2.5 Flash ({'Tools Enabled' if self.tools else 'No Tools'})")
            except Exception as e:
                logger.error(f"âŒ Error initializing Vertex AI: {str(e)}")
                self._model = None
        else:
            logger.warning("âš ï¸  CerebroIA running in fallback mode (no AI)")
    
    def _get_system_instruction(self) -> str:
        """
        Get system instruction with fallback strategy:
        1. Firestore Config (via ConfigLoader) - Dynamic
        2. Local JSON file - Robust Fallback
        3. Code Constant - Last Resort
        """
        instruction = ""
        
        # 1. Try ConfigLoader (Firestore)
        if self.config_loader:
            try:
                # FIX: Correct method name and dictionary access
                personality = self.config_loader.get_juan_pablo_personality()
                instruction = personality.get("system_instruction", "")
                if instruction:
                    logger.info("ğŸ§  Loaded system instruction from Firestore Config")
                    return instruction
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to load prompt from ConfigLoader: {e}")

        # 2. Try JSON File
        try:
            import json
            json_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "core", "personality.json")
            if os.path.exists(json_path):
                with open(json_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    instruction = data.get("system_instruction", "")
                    if instruction:
                        logger.info("ğŸ§  Loaded system instruction from personality.json")
                        return instruction
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to load prompt from JSON: {e}")

        # 3. Fallback to constant
        from app.core.prompts import JUAN_PABLO_SYSTEM_INSTRUCTION
        logger.info("ğŸ§  Loaded system instruction from code constant (Fallback)")
        return JUAN_PABLO_SYSTEM_INSTRUCTION

    def _default_instruction(self) -> str:
        return self._get_system_instruction()
    
    def pensar_respuesta(self, texto: str, context: str = "", prospect_data: Optional[Dict[str, Any]] = None, history: list = [], skip_greeting: bool = False) -> str:
        """
        Generate an intelligent response using Gemini AI with Retry Logic.
        """
        return self._generate_with_retry(texto, context, prospect_data, history, skip_greeting)

    def _create_tools(self) -> Optional[Tool]:
        """
        Create tools for function calling (human handoff).
        Returns: Tool object with function declarations, or None if not available
        """
        if not VERTEX_AI_AVAILABLE:
            return None
        
        try:
            # Define human handoff function
            handoff_function = FunctionDeclaration(
                name="trigger_human_handoff",
                description="Escalate conversation to a human agent when user requests human assistance or query is too complex",
                parameters={
                    "type": "object",
                    "properties": {
                        "reason": {
                            "type": "string",
                            "description": "Reason for handoff (e.g., 'user_request', 'complex_query', 'technical_question')"
                        }
                    },
                    "required": ["reason"]
                }
            )

            # Define catalog search function
            catalog_function = FunctionDeclaration(
                name="search_catalog",
                description="Search for motorcycles in the catalog using a query string. Use this to find prices, specs, and models.",
                parameters={
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "Search query (e.g., 'NKD 125', 'motos deportivas', 'precio de la Victory')"
                        }
                    },
                    "required": ["query"]
                }
            )
            
            # Define credit calculation function
            credit_function = FunctionDeclaration(
                name="calculate_credit_score",
                description="Calculate credit score and financing strategy based on user profile. Returns score, entity, and application link.",
                parameters={
                    "type": "object",
                    "properties": {
                        "ocupacion_y_contrato": {
                            "type": "string",
                            "description": "OcupaciÃ³n y tipo de contrato (e.g., 'Empleado fijo', 'Independiente', 'Pensionado')"
                        },
                        "ingresos_demostrables": {
                            "type": "string",
                            "description": "Nivel de ingresos (e.g., 'Minimo', '1750905')"
                        },
                        "historial_datacredito": {
                            "type": "string",
                            "description": "Estado en DatacrÃ©dito (e.g., 'Al dia', 'Reportado', 'Sin experiencia')"
                        },
                        "mora_y_paz_salvo": {
                            "type": "string",
                            "description": "Detalles de mora (>30 dÃ­as) y Paz y Salvo. Opciones: 'Sin mora', 'Con mora y paz y salvo', 'Con mora sin paz y salvo'"
                        },
                        "gastos_vivienda": {
                            "type": "string",
                            "description": "Gastos de vivienda (e.g., 'Familiar', 'Arriendo 500k')"
                        },
                        "tiene_gas_natural": {
                            "type": "boolean",
                            "description": "Indica si tiene recibo de Gas Natural a su nombre (true o false)"
                        },
                        "plan_celular": {
                            "type": "string",
                            "description": "Tipo de plan de celular (e.g., 'Postpago', 'Prepago')"
                        }
                    },
                    "required": [
                        "ocupacion_y_contrato", 
                        "ingresos_demostrables", 
                        "historial_datacredito", 
                        "mora_y_paz_salvo", 
                        "gastos_vivienda", 
                        "tiene_gas_natural", 
                        "plan_celular"
                    ]
                }
            )

            return Tool(function_declarations=[handoff_function, catalog_function, credit_function])
        except Exception as e:
            logger.error(f"âŒ Error creating tools: {str(e)}")
            return None

    def _generate_with_retry(self, texto: str, context: str, prospect_data: Optional[Dict[str, Any]] = None, history: list = [], skip_greeting: bool = False) -> str:
        """
        Internal generation with exponential backoff.
        """
        if not self._model: return self._fallback_response(texto, history)
        
        max_retries = 3
        base_delay = 2 
        
        import time
        from google.api_core.exceptions import ResourceExhausted, ServiceUnavailable, InvalidArgument

        for attempt in range(max_retries):
            try:
                chat = self._model.start_chat()
                
                full_prompt = f"{self._system_instruction}\n\n"
                
                # Identity Guard
                full_prompt += "Your name is Juan Pablo. NEVER address the user as Juan Pablo.\n"
                
                # Inject prospect data for personalization
                if prospect_data and prospect_data.get("exists"):
                    full_prompt += "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
                    full_prompt += "INFORMACIÃ“N DEL PROSPECTO (CRM):\n"
                    if prospect_data.get("name"):
                        full_prompt += f"- Nombre: {prospect_data['name']}\n"
                    if prospect_data.get("moto_interest"):
                        full_prompt += f"- InterÃ©s en moto: {prospect_data['moto_interest']}\n"
                    if prospect_data.get("summary"):
                        full_prompt += f"- Resumen previo: {prospect_data['summary']}\n"
                    full_prompt += "\nâš ï¸ INSTRUCCIÃ“N: Usa esta informaciÃ³n para personalizar tu saludo y respuesta.\n"
                    full_prompt += "Ejemplo: 'Â¡Hola {nombre}! Vi que te interesa la {moto}...'\n"
                    full_prompt += "Verifica cortÃ©smente si la informaciÃ³n sigue vigente.\n"
                    full_prompt += "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n"
                
                # Inject Chat History (Recent Context)
                if history:
                    full_prompt += "ğŸ“œ HISTORIAL RECIENTE (Ãšltimos mensajes):\n"
                    for msg in history:
                        role_label = "Usuario" if msg['role'] == 'user' else "Juan Pablo"
                        content_safe = str(msg.get('content', '')).replace('\n', ' ')
                        full_prompt += f"- {role_label}: {content_safe}\n"
                    full_prompt += "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n"

                if context:
                    full_prompt += f"RESUMEN CONVERSACIÃ“N ANTERIOR (Largo Plazo):\n{context}\n\n"
                
                # Greeting Bypass Instruction
                if skip_greeting:
                    full_prompt += "\n[SYSTEM: Omit introductory greetings. Respond directly to the user's query as the conversation is ongoing. KEEP IT SHORT.]\n"

                full_prompt += f"Usuario: {texto}\n\nJuan Pablo:"
                
                # 1. Send initial message
                response = chat.send_message(
                    full_prompt,
                    generation_config=GenerationConfig(temperature=0.2, max_output_tokens=1000),
                    timeout=60.0
                )
                
                # 2. Check for Function Call(s)
                candidate = response.candidates[0]
                function_calls = [part.function_call for part in candidate.content.parts if part.function_call]
                
                if function_calls:
                    logger.info(f"âš¡ AI triggered {len(function_calls)} function call(s)")
                    response_parts = []
                    
                    for function_call in function_calls:
                        function_name = function_call.name
                        
                        # A) Human Handoff
                        if function_name == "trigger_human_handoff":
                            reason = function_call.args.get("reason", "unknown")
                            logger.warning(f"ğŸš¨ AI triggered human handoff | Reason: {reason}")
                            # Special case: If handoff is triggered, we can stop immediately or process others.
                            # For safety, we'll return immediately as this overrides other actions.
                            return f"HANDOFF_TRIGGERED:{reason}"
                        
                        # B) Catalog Search
                        elif function_name == "search_catalog":
                            query = function_call.args.get("query", "")
                            logger.info(f"ğŸ” AI searching catalog for: '{query}'")
                            
                            search_results = "No se encontraron resultados."
                            try:
                                if self.catalog_service:
                                    matches = self.catalog_service.search_items(query)
                                    if matches:
                                        search_results = f"EncontrÃ© {len(matches)} motos relacionadas:\n"
                                        for m in matches: 
                                            search_results += f"- {m['name']} ({m['category']}): {m['formatted_price']}\n"
                                            if m.get('specs'):
                                                specs = str(m['specs'])
                                                search_results += f"  Info: {specs}\n"
                                    else:
                                        search_results = "No encontrÃ© motos que coincidan con esa bÃºsqueda. Intenta con otra categorÃ­a o nombre."
                                else:
                                    search_results = "Error: Servicio de catÃ¡logo no disponible."
                            except Exception as e:
                                logger.error(f"âŒ Tool Execution Error (Catalog): {e}")
                                search_results = "Tuve un problema consultando el catÃ¡logo momentÃ¡neamente. Â¿Me podrÃ­as preguntar de nuevo?"
                            
                            logger.info(f"ğŸ“¤ Preparing tool response for '{query}'...") 
                            
                            tool_response_part = Part.from_function_response(
                                name=function_name,
                                response={
                                    "content": search_results 
                                }
                            )
                            response_parts.append(tool_response_part)

                        # C) Credit Calculation
                        elif function_name == "calculate_credit_score":
                            ocupacion = function_call.args.get("ocupacion_y_contrato", "")
                            ingresos = function_call.args.get("ingresos_demostrables", "")
                            datacredito = function_call.args.get("historial_datacredito", "")
                            mora = function_call.args.get("mora_y_paz_salvo", "")
                            vivienda = function_call.args.get("gastos_vivienda", "")
                            gas = function_call.args.get("tiene_gas_natural", False)
                            celular = function_call.args.get("plan_celular", "")
                            
                            logger.info(f"ğŸ’° AI calculating credit score: Ocupacion={ocupacion}, Ingresos={ingresos}, Datacredito={datacredito}, Gas={gas}")
                            
                            credit_result = "No disponible."
                            try:
                                if self.motor_financiero:
                                    result = self.motor_financiero.evaluar_perfil(
                                        ocupacion_y_contrato=ocupacion,
                                        ingresos_demostrables=ingresos,
                                        historial_datacredito=datacredito,
                                        mora_y_paz_salvo=mora,
                                        gastos_vivienda=vivienda,
                                        tiene_gas_natural=gas,
                                        plan_celular=celular
                                    )
                                    credit_result = f"""
âœ… AnÃ¡lisis de CrÃ©dito Completado:
- Score: {result['score']}/1000
- Estrategia: {result['strategy']}
- Entidad Recomendada: {result['entity']}
- Link de Solicitud: {result['link_url']}
- ExplicaciÃ³n: {result['explanation']}

INSTRUCCIÃ“N PARA EL BOT: Usa esta informaciÃ³n para responder al usuario. Si hay link, invÃ­talo a dar clic.
                                    """.strip()
                                else:
                                    credit_result = "Error: Motor financiero no conectado."
                            except Exception as e:
                                logger.error(f"âŒ Tool Execution Error (Credit): {e}")
                                credit_result = "Error calculando el crÃ©dito. Intenta de nuevo."
                            
                            tool_response_part = Part.from_function_response(
                                name=function_name,
                                response={
                                    "content": credit_result
                                }
                            )
                            response_parts.append(tool_response_part)
                    
                    # Send ALL responses back to the model in a single turn
                    if response_parts:
                        logger.info(f"ğŸ“¤ Sending {len(response_parts)} tool responses to AI...")
                        final_response = chat.send_message(response_parts, timeout=60.0)
                        return final_response.text.strip()
                
                # Normal text response
                ai_response = response.text.strip()
                if not ai_response:
                        logger.warning("âš ï¸ Empty AI response")
                        return self._fallback_response(texto, history)
                        
                logger.info(f"âœ… AI response generated ({len(ai_response)} chars)")
                return ai_response
            
            except InvalidArgument as e:
                logger.error(f"âŒ Invalid Argument (400) in AI attempt {attempt+1}: {e}")
                break
                
            except (ResourceExhausted, ServiceUnavailable) as e:
                wait_time = base_delay * (2 ** attempt)
                logger.warning(f"â³ API Limit (429/503). Retrying in {wait_time}s... (Attempt {attempt+1}/{max_retries})")
                time.sleep(wait_time)
                
            except Exception as e:
                logger.error(f"âŒ Error in AI attempt {attempt+1}: {e}", exc_info=True)
                break
        
        logger.error("âŒ Failed to generate AI response after retries")
        return self._fallback_response(texto, history)

    def detect_sentiment(self, text: str) -> str:
        """
        Analyze sentiment of the user message.
        """
        if not self._model: return "NEUTRAL"
        try:
            chat = self._model.start_chat()
            response = chat.send_message(
                f"Analyze the sentiment of this text. Output ONLY one word: POSITIVE, NEUTRAL, NEGATIVE, or ANGRY.\nText: {text}"
            )
            return response.text.strip().upper()
        except:
            return "NEUTRAL"

    def generate_summary(self, conversation_text: str) -> Dict[str, Any]:
        """
        Summarize the conversation and extract structured data.
        """
        if not self._model:
            return {"summary": "", "extracted": {}}
        
        try:
            chat = self._model.start_chat()
            prompt = f"""
Eres Juan Pablo, el asistente virtual experto de Tienda Las Motos.
Tu misiÃ³n es resumir la conversaciÃ³n con el cliente y extraer datos clave.

Analiza esta conversaciÃ³n y genera:
1. Un resumen conciso (1-2 oraciones) del tema principal y datos clave
2. Extrae informaciÃ³n estructurada si estÃ¡ presente

ConversaciÃ³n:
{conversation_text}

Responde en formato JSON:
{{
  "summary": "resumen aquÃ­",
  "extracted": {{
    "name": "nombre si se mencionÃ³",
    "moto_interest": "modelo de moto si se mencionÃ³"
  }}
}}
"""
            response = chat.send_message(prompt)
            response_text = response.text.strip()
            
            import json
            import re
            json_match = re.search(r'```(?:json)?\s*({.*?})\s*```', response_text, re.DOTALL)
            if json_match:
                response_text = json_match.group(1)
            
            result = json.loads(response_text)
            
            if "summary" not in result: result["summary"] = ""
            if "extracted" not in result: result["extracted"] = {}
            
            logger.info(f"ğŸ“ Generated summary with {len(result.get('extracted', {}))} extracted fields")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Error generating summary: {str(e)}")
            return {
                "summary": conversation_text[:200] + "..." if len(conversation_text) > 200 else conversation_text,
                "extracted": {}
            }

    def _fallback_response(self, texto: str, history: list = []) -> str:
        """
        Clean, generic fallback response to avoid hallucinations.
        Uses history to allow basic continuity if AI fails.
        """
        return """
Â¡Hola! Soy Juan Pablo de Tienda Las Motos ğŸï¸

Estoy teniendo un pequeÃ±o problema tÃ©cnico momentÃ¡neo, pero sigo aquÃ­ contigo.

Puedo ayudarte con:
âœ… InformaciÃ³n sobre nuestro catÃ¡logo (NKD, Sport, Victory, MRX)
âœ… Simulaciones de crÃ©dito
âœ… Dudas sobre servicio tÃ©cnico

Â¿PodrÃ­as repetirme tu Ãºltima pregunta o escribirla de otra forma? 
        """.strip()
